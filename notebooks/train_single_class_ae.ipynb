{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/ozgucbertug/Documents/GitHub\")\n",
    "\n",
    "from latentHeuristics.src.ae_templates import mlp_architecture_ala_iclr_18, default_train_params\n",
    "from latentHeuristics.src.autoencoder import Configuration as Conf\n",
    "from latentHeuristics.src.point_net_ae import PointNetAutoEncoder\n",
    "\n",
    "from latentHeuristics.src.in_out import snc_category_to_synth_id, create_dir, PointCloudDataSet, \\\n",
    "                                        load_all_point_clouds_under_folder\n",
    "\n",
    "from latentHeuristics.src.tf_utils import reset_tf_graph\n",
    "from latentHeuristics.src.general_utils import plot_3d_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_out_dir = '../out/model/'          # Use to save Neural-Net check-points etc.\n",
    "top_in_dir = '../ghData/data/' # Top-dir of where point-clouds are stored.\n",
    "\n",
    "experiment_name = 'custom_multi'\n",
    "n_pc_points = 2048                # Number of points per model.\n",
    "bneck_size = 128                  # Bottleneck-AE size\n",
    "ae_loss = 'chamfer'                   # Loss to optimize: 'emd' or 'chamfer'\n",
    "class_name = \"custom_multi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../ghData/data/00000001/asymmetric_0000.ply\n",
      "8000 pclouds were loaded. They belong in 1 shape-classes.\n"
     ]
    }
   ],
   "source": [
    "syn_id = snc_category_to_synth_id()[class_name]\n",
    "class_dir = osp.join(top_in_dir , syn_id)\n",
    "all_pc_data = load_all_point_clouds_under_folder(class_dir, n_threads=8, file_ending='.ply', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load default training parameters (some of which are listed beloq). For more details please print the configuration object.\n",
    "\n",
    "    'batch_size': 50   \n",
    "    \n",
    "    'denoising': False     (# by default AE is not denoising)\n",
    "\n",
    "    'learning_rate': 0.0005\n",
    "\n",
    "    'z_rotate': False      (# randomly rotate models of each batch)\n",
    "    \n",
    "    'loss_display_step': 1 (# display loss at end of these many epochs)\n",
    "    'saver_step': 10       (# over how many epochs to save neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_params = default_train_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder, decoder, enc_args, dec_args = mlp_architecture_ala_iclr_18(n_pc_points, bneck_size)\n",
    "train_dir = create_dir(osp.join(top_out_dir, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = Conf(n_input = [n_pc_points, 3],\n",
    "            loss = ae_loss,\n",
    "            training_epochs = train_params['training_epochs'],\n",
    "            batch_size = train_params['batch_size'],\n",
    "            denoising = train_params['denoising'],\n",
    "            learning_rate = train_params['learning_rate'],\n",
    "            train_dir = train_dir,\n",
    "            loss_display_step = train_params['loss_display_step'],\n",
    "            saver_step = train_params['saver_step'],\n",
    "            z_rotate = train_params['z_rotate'],\n",
    "            encoder = encoder,\n",
    "            decoder = decoder,\n",
    "            encoder_args = enc_args,\n",
    "            decoder_args = dec_args\n",
    "           )\n",
    "conf.experiment_name = experiment_name\n",
    "conf.held_out_step = 5   # How often to evaluate/print out loss on \n",
    "                         # held_out data (if they are provided in ae.train() ).\n",
    "conf.save(osp.join(train_dir, 'configuration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-Trained AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building Encoder\n",
      "encoder_conv_layer_0 conv params =  256 bnorm params =  128\n",
      "Tensor(\"custom_multi_2/Relu:0\", shape=(?, 2048, 64), dtype=float32)\n",
      "output size: 131072 \n",
      "\n",
      "encoder_conv_layer_1 conv params =  8320 bnorm params =  256\n",
      "Tensor(\"custom_multi_2/Relu_1:0\", shape=(?, 2048, 128), dtype=float32)\n",
      "output size: 262144 \n",
      "\n",
      "encoder_conv_layer_2 conv params =  16512 bnorm params =  256\n",
      "Tensor(\"custom_multi_2/Relu_2:0\", shape=(?, 2048, 128), dtype=float32)\n",
      "output size: 262144 \n",
      "\n",
      "encoder_conv_layer_3 conv params =  33024 bnorm params =  512\n",
      "Tensor(\"custom_multi_2/Relu_3:0\", shape=(?, 2048, 256), dtype=float32)\n",
      "output size: 524288 \n",
      "\n",
      "encoder_conv_layer_4 conv params =  32896 bnorm params =  256\n",
      "Tensor(\"custom_multi_2/Relu_4:0\", shape=(?, 2048, 128), dtype=float32)\n",
      "output size: 262144 \n",
      "\n",
      "Tensor(\"custom_multi_2/Max:0\", shape=(?, 128), dtype=float32)\n",
      "Building Decoder\n",
      "decoder_fc_0 FC params =  33024 Tensor(\"custom_multi_2/Relu_5:0\", shape=(?, 256), dtype=float32)\n",
      "output size: 256 \n",
      "\n",
      "decoder_fc_1 FC params =  65792 Tensor(\"custom_multi_2/Relu_6:0\", shape=(?, 256), dtype=float32)\n",
      "output size: 256 \n",
      "\n",
      "decoder_fc_2 FC params =  1579008 Tensor(\"custom_multi_2/decoder_fc_2/BiasAdd:0\", shape=(?, 6144), dtype=float32)\n",
      "output size: 6144 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ../out/model/custom_multi/models.ckpt-500\n"
     ]
    }
   ],
   "source": [
    "load_pre_trained_ae = True\n",
    "restore_epoch = 500\n",
    "if load_pre_trained_ae:\n",
    "    conf = Conf.load(train_dir + '/configuration')\n",
    "    reset_tf_graph()\n",
    "    ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "    ae.restore_model(conf.train_dir, epoch=restore_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not load_pre_trained_ae:\n",
    "    reset_tf_graph()\n",
    "    ae = PointNetAutoEncoder(conf.experiment_name, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not load_pre_trained_ae:\n",
    "    buf_size = 1 # Make 'training_stats' file to flush each output line regarding training.\n",
    "    fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)\n",
    "    train_stats = ae.train(all_pc_data, conf, log_file=fout)\n",
    "    fout.close()"
   ]
  },
  {
   "source": [
    "# Export"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export all latent vectors and labels to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/home/ozgucbertug/Documents/GitHub/latentHeuristics/out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_latent_vectors = np.zeros((len(all_pc_data.point_clouds),128))\n",
    "\n",
    "for i in range(len(all_pc_data.point_clouds)):\n",
    "    pc = all_pc_data.point_clouds[i].reshape((1,2048,3))\n",
    "    all_latent_vectors[i] = ae.transform(pc)\n",
    "\n",
    "if not os.path.exists(out_dir+\"/csv\"):\n",
    "        os.makedirs(out_dir+\"/csv\")\n",
    "\n",
    "vector_fn = out_dir+\"/csv/\"+experiment_name+\"_all_latent_vectors.csv\"\n",
    "np.savetxt(vector_fn, all_latent_vectors, delimiter=',')\n",
    "\n",
    "label_fn = out_dir+\"/csv/\"+experiment_name+\"_all_labels.csv\"\n",
    "np.savetxt(label_fn,all_pc_data.labels.reshape((len(all_pc_data.labels,))), delimiter=',', fmt='%s')"
   ]
  },
  {
   "source": [
    "Export interpolated point cloud (simple_i --> asymmetric_i, spiral_i, lowPoly_i) to ply file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n"
     ]
    }
   ],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def write_ply(points, filename, text=True):\n",
    "    points = [(points[i,0], points[i,1], points[i,2]) for i in range(points.shape[0])]\n",
    "    vertex = np.array(points, dtype=[('x', 'f4'), ('y', 'f4'),('z', 'f4')])\n",
    "    el = PlyElement.describe(vertex, 'vertex', comments=['vertices'])\n",
    "    PlyData([el], text=text).write(filename)\n",
    "\n",
    "if not os.path.exists(out_dir+\"/ply\"):\n",
    "        os.makedirs(out_dir+\"/ply\")\n",
    "\n",
    "n_samples = 4\n",
    "n_interpolate = 10\n",
    "\n",
    "for i in np.linspace(0, 2000, n_samples, endpoint=False):\n",
    "    i = int(i)\n",
    "    simple_asymmetric = ae.interpolate(all_pc_data.point_clouds[4000+i],  all_pc_data.point_clouds[0000+i], steps=n_interpolate)\n",
    "    simple_lowPoly = ae.interpolate(all_pc_data.point_clouds[4000+i],  all_pc_data.point_clouds[2000+i], steps=n_interpolate)\n",
    "    simple_spiral = ae.interpolate(all_pc_data.point_clouds[4000+i],  all_pc_data.point_clouds[6000+i], steps=n_interpolate)\n",
    "    for j in range(n_interpolate+2):\n",
    "        fn = out_dir+\"/ply/simple_asymmetric_%4d_%2d\" % (i,j)\n",
    "        write_ply(simple_asymmetric[j], fn)\n",
    "\n",
    "        fn = out_dir+\"/ply/simple_lowPoly_%4d_%2d\" % (i,j)\n",
    "        write_ply(simple_lowPoly[j], fn)\n",
    "\n",
    "        fn = out_dir+\"/ply/simple_spiral_%4d_%2d\" % (i,j)\n",
    "        write_ply(simple_spiral[j], fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use any plotting mechanism such as matplotlib to visualize the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python2717jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 2.7.17 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}