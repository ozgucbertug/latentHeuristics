--- 
title: Latent Heuristics
description: Ozguc Bertug Capunaman (<a href="okc5048@psu.edu">okc5048@psu.edu</a>)  Shakthi Suresh (<a href="sns5410@psu.edu">sns5410@psu.edu</a>) 
---
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure2.jpg?raw=true)

# Introduction

The gap between linear and discrete nature of computational systems and the reciprocal nature of creative inquiries poses a significant barrier to employing traditional CAD systems in complex design problems. Unlike these systems, Machine Learning (ML) models can be implemented to contribute to the creative process by enabling the users to explore otherwise hidden latent space. This project ist the first stage of a more extensive research, exploring the use of learning-based models in suggestive modeling in creative inquiries. The goal is to develop a Machine Learning (ML) model that is capable of deriving solutions from the latent space based on a problem-specific dataset, given a partial or incomplete design solution generated by the user. Recent works in the area of shape completion offer promising results in reconstructing voxel representations from partial or missing data (Stutz & Geiger, 2020), and we believe a similar approach can be used for the hypothesis generation stage. Such efforts utilize the modular nature of Auto Encoder (AE) networks first to teach latent space representation and reconstruction and then use the trained decoder in another cycle of training to teach latent mapping between partial data and full reconstruction.

By doing so, we hypothesize that these systems can go beyond the role of task executors and take an active role in the design process. Furthermore, by training these learning-based models on problem-specific datasets, we aim to capture design intentions and provide relevant solutions as opposed to generic, cookie-cutter algorithms traditional systems incorporate. We believe that changing our perspective on CAD systems from automation and optimization to idea generation and collaboration can facilitate new human-machine interaction scenarios in creative domains.

# Background

Historically, the field of design has been influenced by new techniques and technologies for design representation and fabrication. Arguably, one of the most influential of such technologies has been CAD since its conceptualization in the early 60s. Even though early visions for CAD were to create a collaborative partner in creative exploration, the course of future research shifted from developing a collaborative partner to “perfect slaves'' poised to liberate designers from arduous tasks [1], This change in perspective crystalised the instrumentalist perspective surrounding the computer aids to design discourse.
 
Current literature that investigates 3D ML models offer promising results in low-dimensional representation and generation of 3D objects by appropriating well established models. Some of the most popular models used in conjunction with 3D data are Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN). These models are used in a wide range of problems from scene segmentation to object classification in different applications such as autonomous vehicles and robotics.

One area that research in geometric learning focuses on is how the geometry is represented within these models. Currently, there are three mainstream representation methods for handling 3D data representation in learning-based models; (1) voxels, (2) point clouds and (3) meshes. The underlying differences between the three methods lie in the application, operation, and representation. Voxel-based approaches are prevalent in ML applications as they are inherently very similar to multidimensional inputs such as images. However, since the representation relies on a grid-based data structure, this approach is limited in its ability to scale. Point clouds, on the other hand, define each point as a set of cartesian x, y, z coordinates. It is one of the most popular ways of representing 3D data as it offers more flexibility with scaling and can easily be worked into mesh geometries using computer graphics algorithms. However, since point clouds are often unorganized in terms of neighborhood information, this representation model needs to be approached carefully, especially in conjunction with ML models. Lastly, the mesh representation stores data in polygons and vertices that make up the faces of a volume. This eliminates the problem of unorganized data structure seen in point clouds. However, since this approach incorporates the most detailed data, the operations can quickly become computationally heavy.
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure2.jpg?raw=true)
# System Architecture
In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data. Dimensionality reduction can be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. The general idea of autoencoders is pretty simple and consists in setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process.
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure1.jpg?raw=true)
The primary approach employed in this research comes from the prior work done by Achlioptas et al. in their paper titled “Learning Representations and Generative Models for 3D Point Clouds” ([`arXiv`](https://arxiv.org/abs/1707.02392) | [`GitHub`](https://github.com/optas/latent_3d_points)), where they explore the data-driven design capabilities of generative models introduced in the previous section, including a VAE architecture. In this study,  the authors were able to reconstruct and generalize the 3D dataset to interpolate and generate 3D shapes by training a deep Auto-Encoder using point cloud data inputs. This architecture learns to encode the input 2048 points (2048x3 coordinates) into a latent vector of size 128 using 1D convolution filters activated by Rectified Linear Units (ReLU) function and consequent max-pool layer. Using fully connected network structure, the decoder network reconstructs the original point cloud from the given latent space vector. In order to negate any negative effects of point cloud permutations, the authors adopted Earth Mover’s Distance (EMD) and Chamfer distance metrics, which are commonly used in 3D learning models due to their permutation-agnostic approach.
While our approach follows a similar path established by Achlioptas et al., we extend the application of this method in latent design space by taking a narrow design space and extrapolating the dataset. As a proof of concept, we focus on a simple vase topology that is addressed by different design approaches and enable the latent design space that places emphasis on subjective design over objective design.

# Dataset
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure0.gif?raw=true)
_Parametric model developed for synthetic data generation._
This model was initially tested using the point cloud dataset provided by the authors, which consists of uniformly sampled point clouds from a subset of the ShapeNet database. This step was necessary to validate that both the operating system and driver dependencies were met. Following the successful training and exploration of the model with the default dataset, we have shifted our focus to developing parametric scripts on Grasshopper for Rhinoceros 3D that is capable of generating the design space for a vase topology. This parametric model is composed of seven distinct parameters that generate a NURBS curve to create a surface of revolution. This surface was later sampled uniformly to generate point cloud data for training the AE network. In order to broadly capture the design space using these seven design parameters, we have generated 3^7=2,187 synthetic data points, sampling each parameter range three times. Since the range of the parameters are tied to each other, this approach resulted in unique designs that vary in height, maximum and minimum radius, global curvature and top and bottom radii.
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure3.jpg?raw=true)
_Series of datapoints generated using the parametric model_
In addition to broadly capturing a design space, we have also developed three additional scripts that apply different local geometric features to these surfaces. These geometric features are corrugation that creates a ridge and groove pattern, asymmetric that deforms the geometry by introducing random attractor points that pull and push the surface, and low polygon that reconstructs the original geometry with planar facets. Each design approach is randomly generated 2000 data points, totaling 8000 distinct geometries together with the simple surface generation.
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure4.jpg?raw=true)
_Architecture_
With these two datasets, our goal is to understand the capabilities of the AE model in learning both local and global geometric features. The first dataset that captures the entire space by sampling all the possible variables is used to explore whether the AE model used in this study is capable of capturing global geometric features of the design space. Within this experiment, we use the reconstruction and interpolation accuracy as our metric for evaluating the success of the AE model. Using the initial surface as the ground truth, we calculate the reconstruction deviation. Similarly, we can investigate how well each parameter is captured by the model using the parametric model interpolation as the ground truth.

## Using AE to explore latent space

VAE offers simplicity, scalable training, and, most importantly for us, the possibility of manipulating and analyzing the outcome. In contrast, GANs and other methods are computationally expensive while offering little to no meaningful control over the output. 
The primary approach comes from the prior work done by Achlioptas et al. in their paper titled “Learning Representations and Generative Models for 3D Point Clouds” (Achlioptas et al., 2018), where they explore the data-driven design capabilities of AE and GAN. By looking at geometric data as point clouds and employing a deep Auto-Encoder, they were able to reconstruct and generalize the 3D dataset to analyze, interpolate and edit 3D shapes. 

![](https://hackernoon.com/hn-images/1*op0VO_QK4vMtCnXtmigDhA.png)
_Architecture_ [3]

In this work, we aim to explore a similar field while emphasizing the possibilities and freedom this architecture can offer in the space of suggestive creative design as opposed to engineering design. While our approach follows a similar path established by Achlioptas et al., we intend to explore the suggestive design subjectively by taking a narrow design space and extrapolating the dataset. For this reason, we propose focusing on one or two categories of shapes and enable suggestive design space that places emphasis on subjective design over objective design.

![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure1.jpg?raw=true)
_Architecture implemented in this work_[4]

We aim to work with point cloud representation over other prevailing methods. Points clouds provide unordered datasets while being computationally lenient, less memory intensive, and through surface reconstruction, offer an easy path to mesh geometries conversions when needed. Other representation models like voxels and meshes were omitted since they lead to computationally heavy models. We believe that point cloud representation offers an optimal middle ground between the memory-intensive voxel and computationally expensive mesh representations.

![](https://hackernoon.com/hn-images/1*yMFJ-7fokU0Xkx89pSFfew.gif)
_Arithmetic on 3D shapes_ [5]


# Dataset

Within the scope of this project, we plan to use the dataset available at Princeton ModelNet (Z. Wu et al., 2015), a collaborative project of online 3D shapes available for research. Princeton ModelNet covers 662 objects with 127,915 unique CAD models
Each model on the ModelNet dataset is labeled and classified by category tags. However, models under each subsets are unlabeled. Since our aim is to not classify the models but rather extract local and global geometric features and fill in the creative space between models to explore different designs, the lack of these labels does not pose any problem.

![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure3.jpg?raw=true)
_Different shape and size of Vases in the dataset_ [6]

![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure4.jpg?raw=true)
_Different categories of Vase shapes to explore design intent of ML model_[7]

In addition to using ModelNet for developing and testing our model, we also aim to explore custom synthetic data generated using parametric modeling tools such as Grasshopper for Rhinoceros 3D. This would allow us to explore how geometric intentions can be learned through a deliberately biased dataset and investigate how well learning-based algorithms can generate problem-specific solutions.

![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure0.gif?raw=true) 
_Various forms of a Vase generated from Rhinoceros 3D to explore custome synthetic data_[8]

# Results
Input/output/Loss values
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure8.gif?raw=true)
![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure9.gif?raw=true)

# Future Work
This work primarily focuses on stage one of a two stage project. Stage one (Geometric Learning) uses a deep autoencoder that utilizes input from our dataset to train the model into learning the geometric patterns and intentions that can then decode it from the latent space. 

In stage two of this project (Hypothesis Generation), we aim to utilize the decoder from stage one to reconstruct partial or incomplete input through the latent space vector. This stage allows  to  learn  shape  completion  using  an  unsupervised  maximum  likelihood  (ML)  loss  by training a new recognition model, a new encoder.

![](https://github.com/ozgucbertug/latentHeuristics/blob/main/docs/Figure2.jpg?raw=true) _Stage one - Geometric Learning and Stage two - Hypothesis Generation_[9]

# References
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/RUBNER/emd.htm


